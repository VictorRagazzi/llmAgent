{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, AsyncOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from imagepig import ImagePig\n",
    "from base64 import b64decode\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import datetime\n",
    "import re\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Tuple\n",
    "import asyncio\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "API_KEY_PIG = \"\"\n",
    "google_key = \"\"\n",
    "weatherAPI_key = \"\"\n",
    "\n",
    "model_id = \"gemini-2.0-flash\"\n",
    "# client = OpenAI(\n",
    "client = AsyncOpenAI(\n",
    "        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    "        api_key=google_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherInfoAgent:\n",
    "    @staticmethod\n",
    "    async def run(location):\n",
    "        api_key = weatherAPI_key\n",
    "        url = f\"http://api.weatherapi.com/v1/current.json?key={api_key}&q={location}&lang=pt\"\n",
    "        response = requests.get(url)\n",
    "        return response.json()\n",
    "    \n",
    "class ImageGenerationAgent:\n",
    "    def __init__(self, openai_api_key, image_api_key):\n",
    "        ImagePig(image_api_key)\n",
    "        self.image_api_key = image_api_key\n",
    "        self.openai_api_key = openai_api_key\n",
    "\n",
    "    async def refine_prompt(self, user_request):\n",
    "        \"\"\"Refina o prompt do usu√°rio usando o modelo Gemini 2.0.\"\"\"\n",
    "        print(\"üé® Refinando prompt...\")\n",
    "        completion = await client.chat.completions.create(\n",
    "            extra_body={},\n",
    "            model=model_id,\n",
    "            messages=[{\"role\": \"user\",\n",
    "                       \"content\": f\"\"\"Transform the following simple description of an image into a detailed prompt for an image generation model. The output should contain only the generated prompt, without any additional explanations or comments. Use rich descriptions, details about colors, textures, lighting, and environment, ensuring that the image is well represented.\n",
    "                            Example:\n",
    "                            Input: 'I would like a picture of a pig wearing a shirt.'\n",
    "                            Output: 'A pink pig with mud on its feet in a pig pen. The pig in question is wearing a textured green shirt. The image is naturally lit from the sky. A farm with details such as a house, crops, and the pig pen itself can be seen around it.' \n",
    "                            Now, generate a detailed prompt for the following description: {user_request}\"\"\"\n",
    "                        }])\n",
    "        refined_prompt = completion.choices[0].message.content\n",
    "        print(f\"‚úÖ Prompt refinado: {refined_prompt}\")\n",
    "        return refined_prompt\n",
    "\n",
    "    async def generate_image(self, prompt):\n",
    "        \"\"\"Gera uma imagem baseada no prompt refinado.\"\"\"\n",
    "        print(\"üñºÔ∏è Gerando imagem...\")\n",
    "        response = requests.post(\n",
    "            \"https://api.imagepig.com/\",\n",
    "            headers={\"Api-Key\": self.image_api_key},\n",
    "            json={\"prompt\": prompt},\n",
    "        )\n",
    "        if response.ok:\n",
    "            image_data = b64decode(response.json()[\"image_data\"])\n",
    "            image_path = Path(\"generated_image.jpeg\")\n",
    "            with image_path.open(\"wb\") as f:\n",
    "                f.write(image_data)\n",
    "            print(f\"‚úÖ Imagem gerada e salva em: {image_path}\")\n",
    "            return image_path\n",
    "        else:\n",
    "            response.raise_for_status()\n",
    "\n",
    "    def show_image(self, image_path):\n",
    "        \"\"\"Exibe a imagem gerada.\"\"\"\n",
    "        with image_path.open(\"rb\") as f:\n",
    "            img = Image.open(f)\n",
    "            img.show()\n",
    "\n",
    "    async def run(self, user_request):\n",
    "        \"\"\"Executa o pipeline de gera√ß√£o de imagens.\"\"\"\n",
    "        refined_prompt = await self.refine_prompt(user_request)\n",
    "        image_path = await self.generate_image(refined_prompt)\n",
    "        self.show_image(image_path)\n",
    "        return image_path\n",
    "    \n",
    "imageGenerationAgent = ImageGenerationAgent(google_key, API_KEY_PIG)\n",
    "get_weather_info = WeatherInfoAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_ANSWER_TOKEN = \"Final Answer:\"\n",
    "OBSERVATION_TOKEN = \"Observation:\"\n",
    "THOUGHT_TOKEN = \"Thought:\"\n",
    "PROMPT_TEMPLATE = \"\"\"Today is {today} and you can use tools to get new information. Answer the question as best as you can using the following tools: \n",
    "\n",
    "{tool_description}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: comment on what you want to do next\n",
    "Action: the action to take, exactly one element of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation repeats N times, use it until you are sure of the answer)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: your final answer to the original input question\n",
    "\n",
    "**Important:** Do not skip steps. Always provide multiple Thoughts before giving the Final Answer.\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {question}\n",
    "Thought: {previous_responses}\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def parse(generated):\n",
    "    if FINAL_ANSWER_TOKEN in generated:\n",
    "        return \"Final Answer\", generated.split(FINAL_ANSWER_TOKEN)[-1].strip()\n",
    "\n",
    "    regex = r\"Action: [\\[]?(.*?)[\\]]?[\\n]*Action Input:[\\s]*(.*)\"\n",
    "    match = re.search(regex, generated, re.DOTALL)\n",
    "\n",
    "    if not match:\n",
    "        return \"Final Answer\", generated.strip()\n",
    "\n",
    "    tool = match.group(1).strip()\n",
    "    tool_input = match.group(2)\n",
    "    return tool, tool_input.strip(\" \").strip('\"')\n",
    "\n",
    "async def decide_next_action(prompt, request, stop_pattern):\n",
    "    generated = await client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": request}\n",
    "        ],\n",
    "        stop = stop_pattern\n",
    "    )\n",
    "    generated_text = generated.choices[0].message.content  # Extraindo o texto gerado\n",
    "    tool, tool_input = await parse(generated_text)\n",
    "    return generated_text, tool, tool_input\n",
    "\n",
    "async def run_llm(request, show_CoT=False):\n",
    "    # global messages\n",
    "    previous_responses = []\n",
    "    max_loops = 15\n",
    "    num_loops = 0\n",
    "    stop_pattern: List[str] = [f'\\n{OBSERVATION_TOKEN}', f'\\n\\t{OBSERVATION_TOKEN}']\n",
    "    prompt_template: str = PROMPT_TEMPLATE\n",
    "    tool_description: str = \"\"\"\n",
    "        ImageGenerationAgent: Generates images based on detailed descriptions.\n",
    "        WeatherInfoAgent: Fetches the current weather information for a given location.\n",
    "    \"\"\"\n",
    "    tool_names: str = \"\"\"\n",
    "        ImageGenerationAgent, WeatherInfoAgent\n",
    "    \"\"\"\n",
    "    tools = {\n",
    "        \"ImageGenerationAgent\": imageGenerationAgent,\n",
    "        \"WeatherInfoAgent\": get_weather_info\n",
    "    }\n",
    "\n",
    "    prompt = prompt_template.format(\n",
    "        today               = datetime.date.today(),\n",
    "        tool_description    = tool_description,\n",
    "        tool_names          = tool_names,\n",
    "        question            = request,\n",
    "        previous_responses = '{previous_responses}'\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== BEGIN! ===\\n\")\n",
    "    # print(prompt.format(previous_responses=''))\n",
    "    while num_loops < max_loops:\n",
    "        num_loops += 1\n",
    "        curr_prompt = prompt.format(previous_responses='\\n'.join(previous_responses))\n",
    "        generated, tool, tool_input = await decide_next_action(curr_prompt, request, stop_pattern)\n",
    "        if show_CoT:  \n",
    "            print(f\"\\n=== Loop Number {num_loops} ===\\n\", flush=True)\n",
    "            print(generated, flush=True)\n",
    "\n",
    "        if tool == 'Final Answer':\n",
    "            if show_CoT:\n",
    "                print(\"\\n=== Final Answer ===\\n\", flush=True)\n",
    "                print(tool_input, flush=True)\n",
    "            return tool_input\n",
    "        if tool not in tool_names:\n",
    "            raise ValueError(f\"Unknown tool: {tool}\")\n",
    "        \n",
    "        tool_result = await tools[tool].run(tool_input)\n",
    "        \n",
    "        # generated += f\"\\n{OBSERVATION_TOKEN} {tool_result}\\n{THOUGHT_TOKEN}\"\n",
    "        response_text = f\"\\n{OBSERVATION_TOKEN} {tool_result}\\n{THOUGHT_TOKEN}\"\n",
    "        generated += response_text\n",
    "\n",
    "        if show_CoT:\n",
    "            print(response_text, flush=True)\n",
    "\n",
    "        previous_responses.append(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== INICIANDO O PROCESSO ===\n",
      "\n",
      "\n",
      "=== Loop Number 1 ===\n",
      "\n",
      "Thought: Okay, let's break down the problem step by step to determine how many apples are left.\n",
      "\n",
      "1. **Start:** You begin with 6 apples.\n",
      "2. **Picking apples:** You pick 5 more apples. Total: 6 + 5 = 11 apples.\n",
      "3. **Eating apples:** You eat 2 apples. Total: 11 - 2 = 9 apples.\n",
      "4. **Giving apples:** You give 1 apple to your sister and 1 to your mother. Total: 9 - 1 - 1 = 7 apples.\n",
      "5. **Making a pie:** You use 3 apples for a pie. Total: 7 - 3 = 4 apples.\n",
      "\n",
      "Therefore, you have 4 apples remaining.\n",
      "\n",
      "Final Answer: 4\n",
      "\n",
      "\n",
      "=== Final Answer ===\n",
      "\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await run_llm(\"Eu comprei 6 ma√ßas, em seguida fui na ro√ßa e colhi mais 5 ma√ßas do p√© e comi 2 enquanto fazia isso.\"\n",
    "    \" Depois de colher, entreguei uma a minha irm√£ e uma a minha m√£e. Em seguida utilizei 3 para fazer uma torta. \"\n",
    "    \"Quantas ma√ßas me sobraram?\",show_CoT=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== INICIANDO O PROCESSO ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Voc√™ sobrou com 4 ma√ßas.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await run_llm(\"Eu comprei 6 ma√ßas, em seguida fui na ro√ßa e colhi mais 5 ma√ßas do p√© e comi 2 enquanto fazia isso. \"\n",
    "    \"Depois de colher, entreguei uma a minha irm√£ e uma a minha m√£e. Em seguida utilizei 3 para fazer uma torta. \"\n",
    "    \"Quantas ma√ßas me sobraram?\",show_CoT=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== INICIANDO O PROCESSO ===\n",
      "\n",
      "\n",
      "=== Loop Number 1 ===\n",
      "\n",
      "Thought: Primeiro, preciso calcular o n√∫mero total de ma√ß√£s que a pessoa tinha. Inicialmente, ela tinha 6 ma√ß√£s compradas. Depois, colheu mais 5. Ent√£o, o total de ma√ß√£s √© 6 + 5 = 11.\n",
      "\n",
      "Em seguida, preciso subtrair as ma√ß√£s que ela comeu, deu para a irm√£, para a m√£e e usou na torta. Ela comeu 2, deu 1 para a irm√£ e 1 para a m√£e, e usou 3 na torta. Ent√£o, o total de ma√ß√£s que sa√≠ram √© 2 + 1 + 1 + 3 = 7.\n",
      "\n",
      "Finalmente, subtraio as ma√ß√£s que sa√≠ram do n√∫mero total de ma√ß√£s: 11 - 7 = 4.\n",
      "\n",
      "Agora, gero uma imagem com 4 ma√ß√£s.\n",
      "Action: ImageGenerationAgent\n",
      "Action Input: Four red apples arranged in a visually appealing way.\n",
      "üé® Refinando prompt...\n",
      "‚úÖ Prompt refinado: Four vibrant, crimson red apples with subtle variations in shape and size, arranged artfully on a dark, polished wooden table. Light reflects off their glossy skins, highlighting small imperfections and subtle color variations, from deep scarlet to hints of orange. One apple has a single, perfectly formed dewdrop clinging to its side. The background is softly blurred, suggesting a rustic kitchen with warm, ambient lighting emanating from a nearby window. A faint, diffused golden glow illuminates the scene, casting gentle shadows that emphasize the apples' roundness and create a sense of depth and three-dimensionality. The texture of the wooden table is visible, with subtle grain patterns and a smooth, reflective surface.\n",
      "\n",
      "üñºÔ∏è Gerando imagem...\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://api.imagepig.com/",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_llm(\u001b[33m\"\u001b[39m\u001b[33mEu comprei 6 ma√ßas, em seguida fui na ro√ßa e colhi mais 5 ma√ßas do p√© e comi 2 enquanto fazia isso. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDepois de colher, entreguei uma a minha irm√£ e uma a minha m√£e. Em seguida utilizei 3 para fazer uma torta. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mQuantas ma√ßas me sobraram? Fa√ßa uma imagem do n√∫mero de ma√ßas da resposta final\u001b[39m\u001b[33m\"\u001b[39m,show_CoT=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      4\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mrun_llm\u001b[39m\u001b[34m(request, show_CoT)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tool \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tool_names:\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown tool: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtool\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m tool_result = \u001b[38;5;28;01mawait\u001b[39;00m tools[tool].run(tool_input)\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# generated += f\"\\n{OBSERVATION_TOKEN} {tool_result}\\n{THOUGHT_TOKEN}\"\u001b[39;00m\n\u001b[32m     76\u001b[39m response_text = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mOBSERVATION_TOKEN\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtool_result\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mTHOUGHT_TOKEN\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mImageGenerationAgent.run\u001b[39m\u001b[34m(self, user_request)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Executa o pipeline de gera√ß√£o de imagens.\"\"\"\u001b[39;00m\n\u001b[32m     58\u001b[39m refined_prompt = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.refine_prompt(user_request)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m image_path = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.generate_image(refined_prompt)\n\u001b[32m     60\u001b[39m \u001b[38;5;28mself\u001b[39m.show_image(image_path)\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m image_path\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mImageGenerationAgent.generate_image\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m image_path\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\torug\\Desktop\\trampo\\Acad√™mico\\Projeto IA GEN\\Codar\\pytorch_env\\Lib\\site-packages\\requests\\models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1019\u001b[39m     http_error_msg = (\n\u001b[32m   1020\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1021\u001b[39m     )\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 429 Client Error: Too Many Requests for url: https://api.imagepig.com/"
     ]
    }
   ],
   "source": [
    "await run_llm(\"Eu comprei 6 ma√ßas, em seguida fui na ro√ßa e colhi mais 5 ma√ßas do p√© e comi 2 enquanto fazia isso. \"\n",
    "    \"Depois de colher, entreguei uma a minha irm√£ e uma a minha m√£e. Em seguida utilizei 3 para fazer uma torta. \"\n",
    "    \"Quantas ma√ßas me sobraram? Fa√ßa uma imagem do n√∫mero de ma√ßas da resposta final\",show_CoT=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== INICIANDO O PROCESSO ===\n",
      "\n",
      "\n",
      "=== Loop Number 1 ===\n",
      "\n",
      "Thought: Para responder a essa pergunta, preciso saber a localiza√ß√£o espec√≠fica dentro de Minas Gerais, pois a temperatura pode variar bastante dependendo da cidade ou regi√£o. Em seguida, usarei a ferramenta de busca do clima para obter a temperatura.\n",
      "\n",
      "Action: WeatherInfoAgent\n",
      "Action Input: Belo Horizonte, Minas Gerais\n",
      "\n",
      "Observation: {'location': {'name': 'Belo Horizonte', 'region': 'Minas Gerais', 'country': 'Brazil', 'lat': -19.9167, 'lon': -43.9333, 'tz_id': 'America/Sao_Paulo', 'localtime_epoch': 1741548485, 'localtime': '2025-03-09 16:28'}, 'current': {'last_updated_epoch': 1741547700, 'last_updated': '2025-03-09 16:15', 'temp_c': 31.1, 'temp_f': 88.0, 'is_day': 1, 'condition': {'text': 'Parcialmente nublado', 'icon': '//cdn.weatherapi.com/weather/64x64/day/116.png', 'code': 1003}, 'wind_mph': 4.3, 'wind_kph': 6.8, 'wind_degree': 270, 'wind_dir': 'W', 'pressure_mb': 1015.0, 'pressure_in': 29.97, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 31, 'cloud': 50, 'feelslike_c': 31.7, 'feelslike_f': 89.0, 'windchill_c': 27.5, 'windchill_f': 81.5, 'heatindex_c': 27.4, 'heatindex_f': 81.3, 'dewpoint_c': 12.9, 'dewpoint_f': 55.2, 'vis_km': 10.0, 'vis_miles': 6.0, 'uv': 0.9, 'gust_mph': 6.1, 'gust_kph': 9.8}}\n",
      "Thought:\n",
      "\n",
      "=== Loop Number 2 ===\n",
      "\n",
      "Thought: A temperatura em Belo Horizonte, Minas Gerais, agora √© de 31.1 graus Celsius. Como a temperatura varia dependendo da regi√£o, √© importante notar que essa √© a temperatura em Belo Horizonte e outras √°reas de Minas Gerais podem ter temperaturas diferentes.\n",
      "Final Answer: A temperatura em Belo Horizonte, Minas Gerais, √© de 31.1 graus Celsius.\n",
      "\n",
      "=== Final Answer ===\n",
      "\n",
      "A temperatura em Belo Horizonte, Minas Gerais, √© de 31.1 graus Celsius.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A temperatura em Belo Horizonte, Minas Gerais, √© de 31.1 graus Celsius.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await run_llm(\"Qual a temperatura em Minas Gerais hoje?\",show_CoT=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
